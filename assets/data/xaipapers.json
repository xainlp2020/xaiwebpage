{
    "local-post-hoc": [
        {
            "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
            "title": "Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures",
            "link": "https://www.aclweb.org/anthology/W18-5403.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "5",
            "nlp_task_1": "Semantics: Sentence Level",
            "explainability": "Example-driven, Provenance, Feature Importance",
            "visualization": "Natural Language, Raw Examples",
            "main_explainability": "example-driven",
            "main_visualization": "raw examples",
            "placement": "1",
            "operations": "Layerwise Relevance Propagation",
            "evaluation_metrics": "Human Evaluation + Informal Examination",
            "parts_covered": ""
        },
        {
            "authors": "Eric Wallace, Shi Feng, Jordan Boyd-Graber",
            "title": "Interpreting  Neural  Networks with  Nearest  Neighbors.",
            "link": "https://www.aclweb.org/anthology/W18-5416.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "10",
            "nlp_task_1": "",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "first derivative saliency",
            "evaluation_metrics": "none",
            "parts_covered": "medium"
        },
        {
            "authors": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",
            "title": "Right  for  the  Right  Reasons: Training  Differentiable  Models  by  Constraining  Their Explanations",
            "link": "https://www.ijcai.org/Proceedings/2017/0371.pdf",
            "year": "2017",
            "venue": "IJCAI",
            "type": "full",
            "citation": "117",
            "nlp_task_1": "",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "Input gradient explanations",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": ""
        },
        {
            "authors": "Pankaj Gupta, Hinrich Schutze",
            "title": "LISA:   Explaining Recurrent  Neural  Network  Judgments  via  Layer-wIse Semantic Accumulation  and Example    to Pattern Transformation.",
            "link": "https://www.aclweb.org/anthology/W18-5418.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "5",
            "nlp_task_1": "Information Extraction",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "Layerwise Semantic Accumulation",
            "evaluation_metrics": "none",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Robert Schwarzenberg, David Harbecke, Vivien Macketanz, Eleftherios Avramidis, Sebastian MÃ¶ller",
            "title": "Train, Sort, Explain: Learning to Diagnose Translation Models.",
            "link": "https://www.aclweb.org/anthology/N19-4006.pdf",
            "year": "2019",
            "venue": "NAACL",
            "type": "short",
            "citation": "3",
            "nlp_task_1": "Interpretability and Analysis of Models for NLP",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "neuron activation",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": "medium"
        },
        {
            "authors": "David Harbecke, Robert Schwarzenberg, Christoph Alt",
            "title": "Learning  Explanations  from Language Data.",
            "link": "https://www.aclweb.org/anthology/W18-5434.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "1",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "computes saliency but with new metrics borrowed from vision",
            "evaluation_metrics": "none",
            "parts_covered": "applies feature saliency using untested metrics from vision (largely untested for NLP)"
        },
        {
            "authors": "David Alvarez-Melis and Tommi S. Jaakkola",
            "title": "A  causal framework  for  explaining  the  predictions  of  black-box sequence-to-sequence  models.",
            "link": "https://www.aclweb.org/anthology/D17-1042.pdf",
            "year": "2017",
            "venue": "EMNLP",
            "type": "full",
            "citation": "66",
            "nlp_task_1": "Machine Translation",
            "explainability": "Surrogate Model",
            "visualization": "Saliency",
            "main_explainability": "surrogate model",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "input perturbation",
            "evaluation_metrics": "none",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Nina Poerner, Hinrich SchÃ¼tze, Benjamin Roth",
            "title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.",
            "link": "https://www.aclweb.org/anthology/P18-1032/",
            "year": "2018",
            "venue": "ACL",
            "type": "full",
            "citation": "16",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Surrogate Model, Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "surrogate model",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "pointing game paradigm + creation of hybrid documents",
            "evaluation_metrics": "Comparison to ground truth",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
            "title": "\"Why  Should  I  Trust  You?\": Explaining the Predictions of Any Classifier.",
            "link": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf",
            "year": "2016",
            "venue": "KDD",
            "type": "full",
            "citation": "2701",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Surrogate Model, Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "surrogate model",
            "main_visualization": "saliency",
            "placement": "1",
            "operations": "Sampling, Word-level Feature Importance and Saliency,",
            "evaluation_metrics": "\"Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on\ntrust and associated tasks. \"\n\n* measure faithfulness of explanations,  For  each prediction on the test set, we generate explanations and\ncompute the fraction of these gold features that are recovered\nby the explanations. We report this recall averaged over all\nthe test instances",
            "parts_covered": "Very High"
        },
        {
            "authors": "Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp",
            "title": "Learning  to  Explain  Entity Relationships  in  Knowledge  Graphs.",
            "link": "https://www.aclweb.org/anthology/P15-1055/",
            "year": "2015",
            "venue": "ACL",
            "type": "full",
            "citation": "44",
            "nlp_task_1": "Information Retrieval and Text Mining",
            "explainability": "Tree Induction, Surrogate Model, Feature Importance",
            "visualization": "Natural Language",
            "main_explainability": "surrogate model",
            "main_visualization": "raw examples",
            "placement": "1",
            "operations": "Information Retrieval-base, it cast the problem as a ranking problem. Given two entities, they search for sentences that mention the two entities, then rank the sentences using some features.",
            "evaluation_metrics": "none",
            "parts_covered": "high"
        },
        {
            "authors": "Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli,\nDmitry Ustalov, Simone Paolo Ponzetto, and Chris Biemann",
            "title": "Unsupervised,  Knowledge- Free,   and   Interpretable   Word   Sense   Disambiguation.",
            "link": "https://www.aclweb.org/anthology/D17-2016.pdf",
            "year": "2017",
            "venue": "EMNLP",
            "type": "demo",
            "citation": "9",
            "nlp_task_1": "Semantics: Lexical",
            "explainability": "Example-driven, Feature Importance",
            "visualization": "Raw Examples, Saliency, Other Visualization Techniques",
            "main_explainability": "Example-driven",
            "main_visualization": "raw examples",
            "placement": "1",
            "operations": "Clustering based on several word features (embeddings, common hypernyms, distributional similarity scores, â¦)\n\nWord-based Saliency",
            "evaluation_metrics": "No evaluation",
            "parts_covered": "argues that a user-interface that provides visualization and supporting word sense disambiguation with additional word features (e.g. hypernyms, related sense, ..)"
        }
    ],
    "local-self": [
        {
            "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
            "title": "Auditing Deep Learning processes through Kernel-based Explanatory Models.",
            "link": "https://www.aclweb.org/anthology/D19-1415.pdf",
            "year": "2019",
            "venue": "EMNLP",
            "type": "full",
            "citation": "0",
            "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
            "explainability": "Example-driven",
            "visualization": "Raw Examples",
            "main_explainability": "example-driven",
            "main_visualization": "raw examples",
            "placement": "2",
            "operations": "layer-wise relevance propagation, surrogate model",
            "evaluation_metrics": "F1",
            "parts_covered": "saliency via layer-wise relevance propagation"
        },
        {
            "authors": "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein",
            "title": "Explainable  Prediction  of Medical  Codes  from  Clinical  Text.",
            "link": "https://www.aclweb.org/anthology/N18-1100.pdf",
            "year": "2018",
            "venue": "NAACL",
            "type": "full",
            "citation": "61",
            "nlp_task_1": "",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "Human Evaluation",
            "parts_covered": "medium"
        },
        {
            "authors": "Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy",
            "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion.",
            "link": "https://www.aclweb.org/anthology/P17-1088.pdf",
            "year": "2017",
            "venue": "ACL",
            "type": "full",
            "citation": "33",
            "nlp_task_1": "",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": "medium"
        },
        {
            "authors": "FrÃ©deric Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, Thomas Demeester",
            "title": "Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?",
            "link": "https://www.aclweb.org/anthology/D18-1365.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "full",
            "citation": "8",
            "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "compare to ground truth",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Malika Aubakirova, Mohit Bansal",
            "title": "Interpreting Neural Networks to Improve Politeness Comprehension.",
            "link": "https://pdfs.semanticscholar.org/331b/2dcd5f6250bd28c6f46cab09b474dce6e9a6.pdf",
            "year": "2016",
            "venue": "EMNLP",
            "type": "short",
            "citation": "22",
            "nlp_task_1": "Computational Social Science and Social Media",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "first derivative saliency (plus maybe, though leaning towards not adding activation cluster as an operation)",
            "evaluation_metrics": "none",
            "parts_covered": "small (heatmap on important words)"
        },
        {
            "authors": "Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli",
            "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
            "link": "https://www.aclweb.org/anthology/D18-1537.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "short",
            "citation": "19",
            "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "(a) attention saliency\n(b) LSTM gating signals",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": ""
        },
        {
            "authors": "Sweta Karlekar, Tong Niu, Mohit Bansal",
            "title": "Detecting Linguistic Characteristics of Alzheimerâs Dementia by Interpreting Neural Models.",
            "link": "https://www.aclweb.org/anthology/N18-2110.pdf",
            "year": "2018",
            "venue": "NAACL",
            "type": "full",
            "citation": "16",
            "nlp_task_1": "NLP Applications",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "first derivative saliency",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": "medium"
        },
        {
            "authors": "Qiuchi Li, Benyou Wang, Massimo Melucci",
            "title": "CNM:  An  Interpretable  Complex- valued  Network  for  Matching.",
            "link": "https://www.aclweb.org/anthology/N19-1420.pdf",
            "year": "2019",
            "venue": "NAACL",
            "type": "full",
            "citation": "7",
            "nlp_task_1": "Question Answering",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "none",
            "evaluation_metrics": "no",
            "parts_covered": "medium (could be good if someone else reviews it)"
        },
        {
            "authors": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal",
            "title": "Generating Token-Level Explanations      for Natural Language Inference",
            "link": "https://www.aclweb.org/anthology/N19-1101.pdf",
            "year": "2019",
            "venue": "NAAcl",
            "type": "short",
            "citation": "1",
            "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
            "explainability": "Feature Importance",
            "visualization": "Raw Trees, Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "(a) multiple instance learning (uses thresholded attention to make token-level predictions)\n(b) input perturbation (LIME and Anchor Explanations)",
            "evaluation_metrics": "Comparison to ground truth",
            "parts_covered": ""
        },
        {
            "authors": "Samuel Carton, Qiaozhu Mei, Paul Resnick",
            "title": "Extractive Adversarial Networks: High-Recall Explanations for Identifying    Personal Attacks in Social Media Posts",
            "link": "https://www.aclweb.org/anthology/D18-1386.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "full",
            "citation": "1",
            "nlp_task_1": "Computational Social Science and Social Media",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "adversarial layer on top of attention",
            "evaluation_metrics": "Comparison to ground truth",
            "parts_covered": ""
        },
        {
            "authors": "Ling Luo, Xiang Ao, Feiyang Pan, Jin Wang, Tong Zhao, Ningzi Yu, Qing He",
            "title": "Beyond Polarity: Interpretable Financial  Sentiment Analysis with Hierarchical  Query- driven Attention.",
            "link": "https://www.ijcai.org/Proceedings/2018/0590.pdf",
            "year": "2018",
            "venue": "IJCAI",
            "type": "full",
            "citation": "16",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "query-driven attention mechanism, \n\nhierarchical query-driven attention",
            "evaluation_metrics": "our model is able to discover spotlights beyond the polarity of a document according to different usersâ requirements, Thus, compared with previous studies, our proposed method is more flexible and explainable",
            "parts_covered": "medium"
        },
        {
            "authors": "Francesco Barbieri, Luis Espinosa-Anke, Jose Camacho-Collados, Steven Schockaert, Horacio Saggion",
            "title": "Interpretable Emoji Prediction via Label-Wise  Attention  LSTMs.",
            "link": "https://www.aclweb.org/anthology/D18-1508.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "short",
            "citation": "8",
            "nlp_task_1": "Computational Social Science and Social Media",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention mechanisms",
            "evaluation_metrics": "mostly based on accuracy metrics -- two interesting metrics are discussed, Accuracy @k and Coverage Error",
            "parts_covered": "medium"
        },
        {
            "authors": "Yichen Jiang, Mohit Bansal",
            "title": "Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.",
            "link": "https://www.aclweb.org/anthology/D19-1455.pdf",
            "year": "2019",
            "venue": "EMNLP",
            "type": "full",
            "citation": "7",
            "nlp_task_1": "Question Answering",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "Attention, NN-based Controller, Neural Modular Networks",
            "evaluation_metrics": "Very basic visual inspection of 10 randomly chosen examples",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Shiou Tian Hsu, Changsung Moon, Paul Jones, Nagiza F. Samatova",
            "title": "An   Interpretable   Generative Adversarial  Approach  to  Classification  of  Latent  Entity Relations in Unstructured Sentences.",
            "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16629/16066",
            "year": "2018",
            "venue": "AAAI",
            "type": "full",
            "citation": "3",
            "nlp_task_1": "Information Extraction",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "Encoder for vector representations and used for reward generation as feedback/labels for a semi-supervised approach to rationale generation and selection.Â",
            "evaluation_metrics": "While the quality of interpretability is difficult to measure without human evaluation, we make the assumption that rationales that lead to better F1 scores are more interpretable than those that lead to lower scores. \n\nWe treat the Encoder as a proxy human reader. Based on this assumption, we evaluated the quality of our rationales by treating attention models as proxy rationale generating models, where rationales are the highest-scoring consecutive ngrams in the attention model. Compared to the proxy rationale models, experiments using rationales generated from our model show a 1.7â¼5.0% improvement in F1 score",
            "parts_covered": "medium"
        },
        {
            "authors": "Yang Yang, Deyu Zhou, Yulan He, Meng Zhang",
            "title": "Interpretable Relevant Emotion Ranking with Event-Driven Attention.",
            "link": "https://www.aclweb.org/anthology/D19-1017.pdf",
            "year": "2019",
            "venue": "EMNLP",
            "type": "full",
            "citation": "0",
            "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "Corpus-level and Document-level  Event-Driven attention, \n\nVector products and Matrix Multiplication, SoftmaxÂ  -- various attention mechanisms",
            "evaluation_metrics": "several accuracy-based metrics + visual assessment",
            "parts_covered": "medium"
        },
        {
            "authors": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency",
            "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
            "link": "https://www.aclweb.org/anthology/P18-1208/",
            "year": "2018",
            "venue": "ACL",
            "type": "full",
            "citation": "48",
            "nlp_task_1": "Language Grounding to Vision, Robotics and Beyond",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "Feature importance where the features are the variables modeling the pair-wise interactions between modalities",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": ""
        },
        {
            "authors": "Nikolaos Pappas, Andrei Popescu-Belis",
            "title": "Explaining  the Stars:  Weighted  Multiple-Instance  Learning  for  Aspect- Based   Sentiment   Analysis.",
            "link": "https://www.aclweb.org/anthology/D14-1052.pdf",
            "year": "2014",
            "venue": "EMNLP",
            "type": "full",
            "citation": "32",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "none",
            "evaluation_metrics": "no",
            "parts_covered": "small"
        },
        {
            "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, and Eduard Hovy",
            "title": "Detecting  and  Explaining  Causes From Text For a Time Series Event.",
            "link": "http://www.cs.cmu.edu/~dongyeok/papers/emnlp17_explain.pdf",
            "year": "2017",
            "venue": "EMNLP",
            "type": "full",
            "citation": "11",
            "nlp_task_1": "NLP Applications",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "Human Evaluation",
            "parts_covered": "medium"
        },
        {
            "authors": "Mantong Zhou, Minlie Huang, Xiaoyan Zhu",
            "title": "Interpretable Reasoning Network for Multi-Relation Question Answering.",
            "link": "https://www.aclweb.org/anthology/C18-1171.pdf",
            "year": "2018",
            "venue": "COLING",
            "type": "full",
            "citation": "11",
            "nlp_task_1": "Question Answering",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "provenance derivation",
            "evaluation_metrics": "none",
            "parts_covered": "small (last mile, exactly the same as MathQA)"
        },
        {
            "authors": "Martin Tutek and Jan Snajder",
            "title": "Iterative Recursive Attention Model for Interpretable Sequence Classification",
            "link": "https://www.aclweb.org/anthology/W18-5427/",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "3",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Feature Importance - Extracted Features",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "none",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Nicolas Garneau, Jean-Samuel Leboeuf, Luc Lamontagne",
            "title": "Predicting   and   interpreting embeddings  for  out  of  vocabulary words  in downstream tasks.",
            "link": "https://www.aclweb.org/anthology/W18-5439.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "2",
            "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention (on words)",
            "evaluation_metrics": "surrogate prediction quality",
            "parts_covered": ""
        },
        {
            "authors": "Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki Okazaki, Kentaro Inui, Masaaki Nagata",
            "title": "Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models.",
            "link": "https://www.aclweb.org/anthology/W18-5410.pdf",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "2",
            "nlp_task_1": "Summarization",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "explainability-aware architecture",
            "evaluation_metrics": "none",
            "parts_covered": "medium"
        },
        {
            "authors": "Junyu Lu, Chenbin Zhang, Zeying Xie, Guang Ling, Tom Chao Zhou, Zenglin Xu",
            "title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection.",
            "link": "https://www.aclweb.org/anthology/P19-1006.pdf",
            "year": "2019",
            "venue": "ACL",
            "type": "short",
            "citation": "1",
            "nlp_task_1": "Dialogue and Interactive Systems",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "Feature Importance",
            "main_visualization": "Saliency",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "none",
            "parts_covered": "medium"
        },
        {
            "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom",
            "title": "Program  Induction  by  Rationale Generation:  Learning  to  Solve  and  Explain  Algebraic Word Problems",
            "link": "https://www.aclweb.org/anthology/P17-1015.pdf",
            "year": "2017",
            "venue": "ACL",
            "type": "full",
            "citation": "44",
            "nlp_task_1": "Question Answering",
            "explainability": "Rule Induction",
            "visualization": "Natural Language, Raw program",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "explainability-aware architecture (??)",
            "evaluation_metrics": "BLEU, perplexity, accuracy",
            "parts_covered": "generates natural language explanations"
        },
        {
            "authors": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung",
            "title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing.",
            "link": "https://www.aclweb.org/anthology/P19-1331/",
            "year": "2019",
            "venue": "ACL",
            "type": "full",
            "citation": "5",
            "nlp_task_1": "Summarization",
            "explainability": "Program Induction",
            "visualization": "Raw program",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "explainability-aware architecture",
            "evaluation_metrics": "Human Evaluation",
            "parts_covered": "generates an explanation and produces prediction from it"
        },
        {
            "authors": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi",
            "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms.",
            "link": "https://www.aclweb.org/anthology/N19-1245.pdf",
            "year": "2019",
            "venue": "NAACL",
            "type": "full",
            "citation": "3",
            "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
            "explainability": "Program Induction",
            "visualization": "Raw program",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "provenance, \"explainability-aware architecture\"",
            "evaluation_metrics": "Informal Examination",
            "parts_covered": "small (\"last mile\")"
        },
        {
            "authors": "Pouya Pezeshkpour, Yifan Tian, Sameer Singh",
            "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.",
            "link": "https://www.aclweb.org/anthology/N19-1337/",
            "year": "2019",
            "venue": "NAACL",
            "type": "full",
            "citation": "2",
            "nlp_task_1": "Information Extraction",
            "explainability": "Rule Induction",
            "visualization": "Raw Rules",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "adversarial modification (identifies the fact that when removed from the KG changes the prediction for a target fact)",
            "evaluation_metrics": "not really, just qualitative examination of the output",
            "parts_covered": "medium"
        },
        {
            "authors": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi",
            "title": "Unsupervised  Discrete  Sentence Representation  Learning  for  Interpretable  Neural  Dialog Generation.",
            "link": "https://www.aclweb.org/anthology/P18-1101.pdf",
            "year": "2017",
            "venue": "ACL",
            "type": "full",
            "citation": "43",
            "nlp_task_1": "Dialogue and Interactive Systems",
            "explainability": "",
            "visualization": "Other Visualization Techniques",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "",
            "evaluation_metrics": "",
            "parts_covered": ""
        },
        {
            "authors": "Chao-Chun Liang, Shih-Hong Tsai, Ting-Yun Chang, Yi-Chung Lin, Keh-Yih Su",
            "title": "A Meaning-based English Math Word  Problem  Solver  with  Understanding,  Reasoning and Explanation.",
            "link": "https://www.aclweb.org/anthology/C16-2032.pdf",
            "year": "2016",
            "venue": "COLING",
            "type": "demo",
            "citation": "7",
            "nlp_task_1": "Question Answering",
            "explainability": "Program Induction, Template-based",
            "visualization": "Natural Language, Raw program",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "annotates with POS tags",
            "evaluation_metrics": "surrogate prediction quality",
            "parts_covered": "builds AST"
        },
        {
            "authors": "Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal",
            "title": "Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension.",
            "link": "https://www.aclweb.org/anthology/P19-1261.pdf",
            "year": "2019",
            "venue": "ACL",
            "type": "full",
            "citation": "4",
            "nlp_task_1": "Question Answering",
            "explainability": "Tree Induction, Provenance",
            "visualization": "Raw Trees, Saliency",
            "main_explainability": "induction",
            "main_visualization": "raw symbolic",
            "placement": "2",
            "operations": "explainability-aware architecture (??)",
            "evaluation_metrics": "none",
            "parts_covered": "small (last mile?)"
        },
        {
            "authors": "Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum",
            "title": "QUINT: Interpretable Question Answering  over  Knowledge  Bases.",
            "link": "https://www.aclweb.org/anthology/D17-2011.pdf",
            "year": "2017",
            "venue": "EMNLP",
            "type": "demo",
            "citation": "15",
            "nlp_task_1": "Question Answering",
            "explainability": "Template-based, Provenance, Example-driven",
            "visualization": "Natural Language, other visualization techniques, Raw Examples",
            "main_explainability": "provenance",
            "main_visualization": "natural Language",
            "placement": "2",
            "operations": "template-based, generates query",
            "evaluation_metrics": "none",
            "parts_covered": "template-based, generates query"
        },
        {
            "authors": "An T. Nguyen, Aditya Kharosekar, Matthew Lease, Byron C. Wallace",
            "title": "Interpretable Joint Graphical Model for   Fact-Checking From Crowds.",
            "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16673/15848",
            "year": "2018",
            "venue": "AAAI",
            "type": "full",
            "citation": "17",
            "nlp_task_1": "",
            "explainability": "Provenance",
            "visualization": "Other Visualization Techniques",
            "main_explainability": "Provenance",
            "main_visualization": "other",
            "placement": "2",
            "operations": "provenance, \"explainability-aware architecture\"",
            "evaluation_metrics": "Performed user study. The study showed that the explanations improved user satisfaction and trust",
            "parts_covered": "entire process"
        },
        {
            "authors": "Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba",
            "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
            "link": "https://www.aclweb.org/anthology/P19-1081.pdf",
            "year": "2019",
            "venue": "ACL",
            "type": "full",
            "citation": "10",
            "nlp_task_1": "Dialogue and Interactive Systems",
            "explainability": "Provenance",
            "visualization": "Other Visualization Techniques",
            "main_explainability": "provenance",
            "main_visualization": "other",
            "placement": "2",
            "operations": "attention",
            "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
            "parts_covered": "small"
        },
        {
            "authors": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher",
            "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
            "link": "https://www.aclweb.org/anthology/P19-1487.pdf",
            "year": "2019",
            "venue": "ACL",
            "type": "full",
            "citation": "12",
            "nlp_task_1": "Question Answering",
            "explainability": "Surrogate Model",
            "visualization": "Natural Language",
            "main_explainability": "Surrogate Model",
            "main_visualization": "Natural Language",
            "placement": "2",
            "operations": "Use pretrained language model  that is finetuned on human-provided explanations",
            "evaluation_metrics": "Human Evaluation + Comparison to ground truth + Informal Examination",
            "parts_covered": ""
        },
        {
            "authors": "Hui Liu, Qingyu Yin, William Yang Wang",
            "title": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification.",
            "link": "https://www.aclweb.org/anthology/P19-1560.pdf",
            "year": "2019",
            "venue": "ACL",
            "type": "full",
            "citation": "9",
            "nlp_task_1": "",
            "explainability": "Surrogate Model",
            "visualization": "Natural Language",
            "main_explainability": "Surrogate Model",
            "main_visualization": "Natural Language",
            "placement": "2",
            "operations": "explainability-aware architecture (??)",
            "evaluation_metrics": "BLEU, human evaluation",
            "parts_covered": "generates prediction from generated explanation"
        },
        {
            "authors": "Alona Sydorova, Nina Porner, Benjamin Roth",
            "title": "Interpretable Question Answering on Knowledge Bases and Text.",
            "link": "https://www.aclweb.org/anthology/P19-1488.pdf",
            "year": "2019",
            "venue": "ACL",
            "type": "full",
            "citation": "1",
            "nlp_task_1": "Question Answering",
            "explainability": "Surrogate Model, Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "surrogate model",
            "main_visualization": "saliency",
            "placement": "2",
            "operations": "attention, input perturbation",
            "evaluation_metrics": "Human Evaluation",
            "parts_covered": "medium"
        }
    ],
    "global-post-hoc": [
        {
            "authors": "Reid Pryzant, Sugato Basu, Kazoo Sone",
            "title": "Interpretable Neural Architectures for Attributing an Adâs Performance to its Writing Style.",
            "link": "https://www.aclweb.org/anthology/W18-5415/",
            "year": "2018",
            "venue": "EMNLP",
            "type": "workshop",
            "citation": "4",
            "nlp_task_1": "Computational Social Science and Social Media",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "3",
            "operations": "class activation mapping + select top k n-grams",
            "evaluation_metrics": "Informal examination",
            "parts_covered": "medium"
        },
        {
            "authors": "Reid Pryzant, Kelly Shen, Dan Jurafsky, Stefan Wager",
            "title": "Deconfounded Lexicon Induction for Interpretable Social Science.",
            "link": "https://nlp.stanford.edu/pubs/pryzant2018lexicon.pdf",
            "year": "2018",
            "venue": "NAACL",
            "type": "full",
            "citation": "6",
            "nlp_task_1": "Computational Social Science and Social Media",
            "explainability": "Feature Importance",
            "visualization": "Saliency",
            "main_explainability": "feature importance",
            "main_visualization": "saliency",
            "placement": "3",
            "operations": "attention",
            "evaluation_metrics": "none",
            "parts_covered": "medium-high"
        },
        {
            "authors": "Ninghao Liu, Xiao Huang, Jundong Li, Xia Hu",
            "title": "On   Interpretation   of   Network Embedding via Taxonomy Induction",
            "link": "https://dl.acm.org/doi/pdf/10.1145/3219819.3220001",
            "year": "2018",
            "venue": "KDD",
            "type": "full",
            "citation": "8",
            "nlp_task_1": "Computational Social Science and Social Media",
            "explainability": "Surrogate Model",
            "visualization": "Other Visualization Techniques",
            "main_explainability": "surrogate model",
            "main_visualization": "other",
            "placement": "3",
            "operations": "hierarchical clustering on embeddings to infer taxonomy",
            "evaluation_metrics": "Comparison to ground truth",
            "parts_covered": ""
        }
    ],
    "global-self": [
        {
            "authors": "Nicolas Prollochs, Stefan Feuerriegel, Dirk Neumann",
            "title": "Learning Interpretable Negation Rules via Weak Supervision at Document   Level: A Reinforcement Learning Approach.",
            "link": "https://www.aclweb.org/anthology/N19-1038.pdf",
            "year": "2019",
            "venue": "NAACL",
            "type": "short",
            "citation": "4",
            "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
            "explainability": "Rule Induction",
            "visualization": "Raw Rules",
            "main_explainability": "induction",
            "main_visualization": "symbolic rep",
            "placement": "4",
            "operations": "none",
            "evaluation_metrics": "human evaluation",
            "parts_covered": "learns DFA rules"
        }
    ]
}